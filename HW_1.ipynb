{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1667659277719,
     "user": {
      "displayName": "Boris Zhestkov",
      "userId": "15589718157134474454"
     },
     "user_tz": -180
    },
    "id": "l-iXHAA1Fz1p"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RuSxPbblHJK1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 = 5.235171359579289\n",
      "x2 = 0.7641245785548506\n"
     ]
    }
   ],
   "source": [
    "# Task 1_HW\n",
    "# Find the roots of square equation by gradient descent\n",
    "# x ** 2 - 6 * x + 4 = 0\n",
    "import numpy as np\n",
    "\n",
    "def grad_f(x):\n",
    "  return 2*x - 6\n",
    "\n",
    "def f(x):\n",
    "  return x**2 - 6*x + 4\n",
    "\n",
    "def grad_descent(x_init, learning_rate, precision):\n",
    "    x_old = x_init\n",
    "    x_new = x_old - learning_rate*grad_f(x_old)\n",
    "    #print(x_old, x_new, f(x_new))\n",
    "    while f(x_new) > precision:\n",
    "        x_old = x_new\n",
    "        x_new = x_old - learning_rate*grad_f(x_old)\n",
    "        #print(x_old, x_new, f(x_new))\n",
    "    return x_new\n",
    "\n",
    "root1 = grad_descent(6, 0.001, 0.0001)\n",
    "root2 = grad_descent(-2, 0.001, 0.0001)\n",
    "print('x1 =', root1)\n",
    "print('x2 =', root2)\n",
    "\n",
    "# Если мы не возьмем очень маленький learning_rate, то сойдется за достойное количество шагов\n",
    "# По-хорошему начальную точку тоже важно брать адекватно и не очень большую. \n",
    "# Выбор точки является важным действией, когда у вас несколько экстремумов у функции и вы плохим выбором точки можете в итоге попасть в локальный минимум\n",
    "# Можно брать с двух разных сторон от центра, ли бо искать центр обычным градиентным спуском с условием abs(x_new - x_old) > precision\n",
    "# А после брать x_init длz x1, как random(-inf; xцентр) и x2 = random (xцентр; +inf)\n",
    "# Можно подставить любые a и b и с, => производная от этой штуки ax + b и делаем то же самое\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 _ HW\n",
    "\n",
    "import torch\n",
    "\n",
    "# Будем использовать активацию с помощью сигмоиды\n",
    "# Можно подставить любую функцию активации, так как это было написано на семинаре\n",
    "# Так как мы используем pytorch меняем метод dot на matmul\n",
    "# Не стал использовать функции написанные на занятии sigmoid_backward, написал сам\n",
    "\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = torch.randn(output_size, input_size, requires_grad=True)\n",
    "        self.bias = torch.randn(output_size, requires_grad=True)\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.add(torch.matmul(self.weights, input), self.bias)\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        forw = self.forward(input)\n",
    "        grad_lin = grad_output * torch.sigmoid(forw) * (1 - torch.sigmoid(forw))\n",
    "        grad_input = torch.matmul(self.weights.t(), grad_lin)\n",
    "        self.grad_weights = torch.matmul(grad_lin, input.t())\n",
    "        self.grad_bias = torch.sum(grad_lin, dim=0)\n",
    "        return grad_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Task 3_HW\n",
    "\n",
    "# Написать 2 оптимизатора адаптивных\n",
    "\n",
    "import math\n",
    "\n",
    "class ADAgrad:\n",
    "    def __init__(self, model: Model, lr= 0.0001):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.accumulated_w = None\n",
    "        self.accumulated_b = None\n",
    "        \n",
    "    def step(self):\n",
    "        self.accumulated_w += self.model.grad_weights ** 2\n",
    "        adapt_lr = self.lr / math.sqrt(self.accumulated_w)\n",
    "        self.model.weights = self.model.weights - adapt_lr * self.model.grad_weights\n",
    "        \n",
    "        self.accumulated_b += self.model.grad_bias ** 2\n",
    "        adapt_lr = self.lr / math.sqrt(self.accumulated_b)\n",
    "        self.model.bias = self.model.bias - adapt_lr * self.model.grad_bias\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.model._clear_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "class RMSProp:\n",
    "    def __init__(self, model: Model, lr= 0.0001, rho = 0.9):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.rho = 0.9\n",
    "        self.accumulated_w = None\n",
    "        self.accumulated_b = None\n",
    "        \n",
    "    def step(self):\n",
    "        self.accumulated_w += self.rho * self.accumulated_w + (1 - self.rho) * (self.model.grad_weights) ** 2\n",
    "        adapt_lr = self.lr / math.sqrt(self.accumulated_w)\n",
    "        self.model.weights = self.model.weights - adapt_lr * self.model.grad_weights\n",
    "        \n",
    "        self.accumulated_b = self.rho * self.accumulated_b + (1 - self.rho) * self.model.grad_bias ** 2\n",
    "        adapt_lr = self.lr / math.sqrt(self.accumulated_b)\n",
    "        self.model.bias = self.model.bias - adapt_lr * self.model.grad_bias\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.model._clear_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2swiHK-HIOq"
   },
   "outputs": [],
   "source": [
    "# Task 2\n",
    "# Realize forward and backward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1667663676928,
     "user": {
      "displayName": "Boris Zhestkov",
      "userId": "15589718157134474454"
     },
     "user_tz": -180
    },
    "id": "ibFFthYnHIlh"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_backward(da, x):\n",
    "    sig = sigmoid(x)\n",
    "    \n",
    "    return da * sig * (1 - sig)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0., x)\n",
    "\n",
    "def relu_backward(da, x):\n",
    "    da = np.array(da, copy = True)\n",
    "    da[x <= 0] = 0;\n",
    "    return da;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1667663680426,
     "user": {
      "displayName": "Boris Zhestkov",
      "userId": "15589718157134474454"
     },
     "user_tz": -180
    },
    "id": "gvMKpB5WFz1z"
   },
   "outputs": [],
   "source": [
    "def mse_loss(t, y):\n",
    "    return (t - y) ** 2\n",
    "\n",
    "def d_mse_loss(t, y):\n",
    "    return 2 * (y - t) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1667663684513,
     "user": {
      "displayName": "Boris Zhestkov",
      "userId": "15589718157134474454"
     },
     "user_tz": -180
    },
    "id": "-qAVeaVcFz10"
   },
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, n_inp, n_out, activation='sigmoid'):\n",
    "        self.w = np.random.randn(n_out, n_inp) * 0.1\n",
    "        self.b = np.random.randn(n_out, 1) * 0.1\n",
    "        if activation == 'sigmoid':\n",
    "            self.activ = sigmoid\n",
    "        if activation == 'relu':\n",
    "            self.activ = relu\n",
    "        elif activation == 'None':\n",
    "            self.activ = None\n",
    "        else:\n",
    "            raise Exception(f'Unknown activation \"{activation}\"')\n",
    "        self._clear_state()\n",
    "\n",
    "    def _clear_state(self):\n",
    "        self.lin = None\n",
    "        self.inp = None\n",
    "        self.d_w = None\n",
    "        self.d_b = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.inp = x\n",
    "        self.lin = np.dot(self.w, x) + self.b\n",
    "        activ = self.activ(self.lin) if self.activ is not None else self.lin\n",
    "\n",
    "        return activ\n",
    "\n",
    "    def backward(self, grad): # grad = d L / d z    Dout \n",
    "        # grad * dz / d lin\n",
    "        if self.activ == sigmoid:\n",
    "            grad_lin = sigmoid_backward(grad, self.lin) \n",
    "        elif self.activ == relu:\n",
    "            grad_lin = relu_backward(grad, self.lin)\n",
    "        else:\n",
    "            grad_lin = grad\n",
    "        # grad_lin * d lin / d w \n",
    "        m = self.inp.shape[1]\n",
    "        self.d_w = np.dot(grad_lin, self.inp.T) / m    # d_in dOut\n",
    "        # grad_lin * d lin / d b \n",
    "        self.d_b = np.sum(grad_lin, axis=1, keepdims=True) / m\n",
    "\n",
    "        grad = np.dot(self.w.T, grad_lin)\n",
    "\n",
    "        return grad\n",
    "\n",
    "# pred = model(x)\n",
    "# loss = criterion(pred, target)\n",
    "# grad = d loss / d pred\n",
    "# model.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1667663288846,
     "user": {
      "displayName": "Boris Zhestkov",
      "userId": "15589718157134474454"
     },
     "user_tz": -180
    },
    "id": "jUZcVU8z2T-t"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, arch: Tuple[Tuple[int, int]], activation):\n",
    "        self.layers = []\n",
    "        for i, p in enumerate(arch):\n",
    "            self.layers.append(\n",
    "                LinearLayer(p[0], p[1], \n",
    "                            activation=activation if i < len(arch)-1 else 'None')\n",
    "                )\n",
    "        self._clear_state()\n",
    "    \n",
    "    def _clear_state(self):\n",
    "        for l in self.layers:\n",
    "            l._clear_state()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "        return grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmLRIBk4Fz12"
   },
   "outputs": [],
   "source": [
    "# Task 3\n",
    "# Realize SGD Momentum optimizer\n",
    "# velocity = momentum * velocity - lr * gradient\n",
    "# w = w + velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkj0NKbQWX_9"
   },
   "outputs": [],
   "source": [
    "#для одного слоя\n",
    "class SGDMomentum:\n",
    "    def __init__(self, model: LinearLayer, lr=0.001, momentum=0.99):\n",
    "        self.lr = lr\n",
    "        self.m = momentum\n",
    "        self.model = model\n",
    "\n",
    "        self.vel_w = np.zeros_like(model.w)\n",
    "        self.vel_b = np.zeros_like(model.b)\n",
    "\n",
    "    def step(self):\n",
    "        self.vel_w = self.m * self.vel_w - self.lr * self.model.d_w\n",
    "        self.vel_b = self.m * self.vel_b - self.lr * self.model.d_b\n",
    "\n",
    "        self.model.w += self.vel_w\n",
    "        self.model.b += self.vel_b\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1667663869197,
     "user": {
      "displayName": "Boris Zhestkov",
      "userId": "15589718157134474454"
     },
     "user_tz": -180
    },
    "id": "YRCDRdKNn8qs"
   },
   "outputs": [],
   "source": [
    "#для всей модели\n",
    "class SGDMomentum:\n",
    "    def __init__(self, model: Model, lr= 0.0001, momentum=0.0):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.m = momentum\n",
    "        self.vel = [[np.zeros_like(layer.w), \n",
    "                     np.zeros_like(layer.b)] for layer in model.layers]\n",
    "\n",
    "    def step(self):\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            self.vel[i][0] = self.vel[i][0] * self.m - self.lr * layer.d_w \n",
    "            self.vel[i][1] = self.vel[i][1] * self.m - self.lr * layer.d_b \n",
    "            layer.w += self.vel[i][0]\n",
    "            layer.b += self.vel[i][1]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.model._clear_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZPKwvE-Fz15"
   },
   "outputs": [],
   "source": [
    "# pred = model(x)\n",
    "# loss = criterion(pred, target)\n",
    "# grad = d loss / d pred\n",
    "# model.backward(grad)\n",
    "# optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXoxTHw5Fz16"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1667662436380,
     "user": {
      "displayName": "Boris Zhestkov",
      "userId": "15589718157134474454"
     },
     "user_tz": -180
    },
    "id": "AN-E_lK_Fz18"
   },
   "outputs": [],
   "source": [
    "x = np.random.uniform(-2, 2, 20000)\n",
    "y = x**2 + np.random.randn()*0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45957,
     "status": "ok",
     "timestamp": 1667663918025,
     "user": {
      "displayName": "Boris Zhestkov",
      "userId": "15589718157134474454"
     },
     "user_tz": -180
    },
    "id": "j2aBvwzyFz18",
    "outputId": "06d14184-f677-46d2-f615-e57a7f6f3590"
   },
   "outputs": [],
   "source": [
    "model = Model(((1, 100), (100, 1)), activation='relu')\n",
    "optim = SGDMomentum(model)\n",
    "for e in range(20):\n",
    "    for i, (val, t) in enumerate(zip(x, y)):\n",
    "        optim.zero_grad()\n",
    "        pred = model.forward(np.array([[val]]))\n",
    "        loss = mse_loss(t, pred)\n",
    "        grad = d_mse_loss(t, pred)\n",
    "        model.backward(grad)\n",
    "        optim.step()\n",
    "        \n",
    "    print(e, model.forward([[1]]), model.forward([[2]]), model.forward([[-1]]), model.forward([[-2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5jAO8pVFz19"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOOY8douFz1-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zROVnJAcFz1-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QWQyu5NFz1-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
